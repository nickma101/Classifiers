{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning - Neutrality Classifier\n",
    "   \n",
    "In this notebook, the manual content analysis data is used to train and evaluate a classifier that assesses the neutrality of an article.   \n",
    "The process includes data preperation, feature selection, and the evaluation and comparison of different types of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, ShuffleSplit, GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "#import German language model\n",
    "import de_core_news_md\n",
    "#define nlp pipe\n",
    "nlp = de_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578 250 150 98\n"
     ]
    }
   ],
   "source": [
    "#read in the manually coded data\n",
    "mca = read_excel(\"mca_data.xlsx\")\n",
    "\n",
    "#read in the article data\n",
    "#first Dataset A\n",
    "articles = read_excel(\"sample.xlsx\")\n",
    "#then Dataset B\n",
    "articles_online = read_excel(\"sample_online_new.xlsx\")\n",
    "#then Dataset C\n",
    "articles2 = read_excel(\"Dataset C.xlsx\")\n",
    "print(len(mca), len(articles), len(articles_online), len(articles2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add the two print dfs together\n",
    "articles = articles.append(articles2)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Date</th>\n",
       "      <th>Length</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2471</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>291 words</td>\n",
       "      <td>Die Demontage einer Ministerin</td>\n",
       "      <td>Maximilian Plück Armin Laschet brauchte am Don...</td>\n",
       "      <td>Plück, Maximilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6803</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>418 words</td>\n",
       "      <td>Microsoft mahnt Berlin ab Der US-Konzern verla...</td>\n",
       "      <td>Gegenwind für die Berliner Beauftragte für Dat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>779</td>\n",
       "      <td>Die Welt</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>595 words</td>\n",
       "      <td>Bolsonaro verliert die Nerven; Brasiliens Staa...</td>\n",
       "      <td>Jair Bolsonaro sieht müde aus, mehrfach hustet...</td>\n",
       "      <td>Tobias Käufer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3110</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>287 words</td>\n",
       "      <td>Anklage im Mordfall Lübcke</td>\n",
       "      <td>Die Bundesanwaltschaft hat Anklage im Fall des...</td>\n",
       "      <td>AFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3266</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-04-25</td>\n",
       "      <td>198 words</td>\n",
       "      <td>Maskenpflicht auchin Schulen?; Bundesbildungsm...</td>\n",
       "      <td>Bundesbildungsministerin Anja Karliczek (CDU) ...</td>\n",
       "      <td>dpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>738</td>\n",
       "      <td>Die Welt</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>931 words</td>\n",
       "      <td>Einsame Spitze; Das Aus der SAP-Chefin Morgan ...</td>\n",
       "      <td>Doppelspitzen erfüllen nur selten ihren Zweck,...</td>\n",
       "      <td>Klaus Boldt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1122</td>\n",
       "      <td>Die Welt</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>872 words</td>\n",
       "      <td>EU-Solidarität, von allen für alle</td>\n",
       "      <td>Statt gemeinsam die Seuche zu bekämpfen, wurde...</td>\n",
       "      <td>Jürgen Rüttgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5298</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>611 words</td>\n",
       "      <td>Parkplatz-Kneipen erlaubt; Stadt kommt Wirten ...</td>\n",
       "      <td>Ein Bier, wo bisher das Auto stand? Schnitzel ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4761</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>705 words</td>\n",
       "      <td>Bürgerrechte gelten auch im Notstand; Grüne be...</td>\n",
       "      <td>Landkreis - Die Zuwächse der Grünen sind zulet...</td>\n",
       "      <td>IRIS HILBERTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1538</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>652 words</td>\n",
       "      <td>CDU fordert Rückbau von zwei Umweltspuren; Tes...</td>\n",
       "      <td>Hendrik Gaasterland Düsseldorf Die CDU-Fraktio...</td>\n",
       "      <td>Gaasterland, Hendrik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5045</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>427 words</td>\n",
       "      <td>Aiwanger plante Notunterkunft; Wirtschaftsmini...</td>\n",
       "      <td>München - Wirtschaftsminister Hubert Aiwanger ...</td>\n",
       "      <td>LISA SCHNELL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>573</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>92 words</td>\n",
       "      <td>Volksentscheid über Cannabis-Legalisierung</td>\n",
       "      <td>Wellington Neuseeland entscheidet im September...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5591</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>734 words</td>\n",
       "      <td>Die Kontrolle über den Text; Der BGH fällt zwe...</td>\n",
       "      <td>Als die Westdeutsche Allgemeine Zeitung (WAZ) ...</td>\n",
       "      <td>WOLFGANG JANISCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5364</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>390 words</td>\n",
       "      <td>Es gibt nix Feiners wia a Schweiners!; Bayernp...</td>\n",
       "      <td>Grafing/Ebersberg - Nach dem Motto ,,Gleiches ...</td>\n",
       "      <td>WKB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>815</td>\n",
       "      <td>Die Welt</td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>1255 words</td>\n",
       "      <td>\"Mit künstlicher Beatmung wird viel Geld gemac...</td>\n",
       "      <td>Jürgen Wasem ist einer der bekanntesten Gesund...</td>\n",
       "      <td>Kaja Klapsa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3401</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>1084 words</td>\n",
       "      <td>Eine Friseurin wird zur Freiheitskämpferin</td>\n",
       "      <td>Wenn wir ans Telefon gehen, haben wir auch geö...</td>\n",
       "      <td>Frank Herrmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2192</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>318 words</td>\n",
       "      <td>Zwei Beutelspender für den Burgacker</td>\n",
       "      <td>Viersen (tre) Mit zwei Kotbeutelspendern möcht...</td>\n",
       "      <td>Treffer, Bianca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5156</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-16</td>\n",
       "      <td>513 words</td>\n",
       "      <td>Kür nach Kampfkandidatur; Stefan Ziegler führt...</td>\n",
       "      <td>Trudering/Riem - Mit gebündelten Kräften haben...</td>\n",
       "      <td>ILONA GERDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4437</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>312 words</td>\n",
       "      <td>,,Vollkommener Schmarrn'; Kirchseeon setzt Ger...</td>\n",
       "      <td>Kirchseeon - Kindergärten und Tagesstätten sin...</td>\n",
       "      <td>AJU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5907</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>841 words</td>\n",
       "      <td>Einsamer Abschied; Jakob Hartl hat am Donnerst...</td>\n",
       "      <td>Nandlstadt - Jakob Hartl hat am Donnerstag nac...</td>\n",
       "      <td>KATHARINA AURICH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4917</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>310 words</td>\n",
       "      <td>Schaden für die Meinungsfreiheit; 700 afrikani...</td>\n",
       "      <td>In der Kontroverse um den afrikanischen Histor...</td>\n",
       "      <td>SONJA ZEKRI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2045</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>169 words</td>\n",
       "      <td>Nicht mit zweierlei Maß!</td>\n",
       "      <td>Man kann einerseits verstehen, warum Friedrich...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5690</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>577 words</td>\n",
       "      <td>EUROPA; Die Öffnung ist zwingend</td>\n",
       "      <td>Als die Not immer größer wurde, fiel den EU-St...</td>\n",
       "      <td>VON JENS SCHNEIDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6878</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>858 words</td>\n",
       "      <td>Verlängerter Arm der Regierung Die Reform des ...</td>\n",
       "      <td>Es sind Stücke aus dem Tollhaus. Der neue Mite...</td>\n",
       "      <td>Reinhart Bünger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1842</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>278 words</td>\n",
       "      <td>Stadt verzichtet auf Terrassengebühr; Gastwirt...</td>\n",
       "      <td>Viersen (mrö) Die Stadt Viersen wird für den Z...</td>\n",
       "      <td>Röse, Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4863</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>754 words</td>\n",
       "      <td>Alte Garde, aber nicht von gestern; Monika Sch...</td>\n",
       "      <td>Planegg - Zwölf von 24 Gemeinderäten - ohne de...</td>\n",
       "      <td>VON RAINER RUTZRAINER RUTZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1607</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>60 words</td>\n",
       "      <td>CDU will mehr Sicherheit für den Römerweg</td>\n",
       "      <td>Erkrath (hup) Die CDU setzt sich für einen bes...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3337</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>342 words</td>\n",
       "      <td>Rauchen und Grillen verboten</td>\n",
       "      <td>Das schöne Wetter lockt in diesen Tagen viele ...</td>\n",
       "      <td>wed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5843</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>825 words</td>\n",
       "      <td>Party, aber bitte mit Maske; In New York wächs...</td>\n",
       "      <td>New York - Wenn man nach New York City blickt,...</td>\n",
       "      <td>CHRISTIAN ZASCHKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4338</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>239 words</td>\n",
       "      <td>,,Kümmerer' für Moosburg; CSU-Fraktion fordert...</td>\n",
       "      <td>Moosburg - Die ,,aktive Unterstützung von Moos...</td>\n",
       "      <td>AXKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4513</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>507 words</td>\n",
       "      <td>Aslyunterkunft unter Quarantäne; Sicherheitsdi...</td>\n",
       "      <td>Erding - Die Forsterner Grünen kritisieren in ...</td>\n",
       "      <td>FLO, TDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2691</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>109 words</td>\n",
       "      <td>Bundesregierung: Balearen-Urlaub nicht abschr...</td>\n",
       "      <td>Berlin (dpa) Urlauber können im Sommer unter U...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3082</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>499 words</td>\n",
       "      <td>Sind Unis digital zu schlecht aufgestellt?</td>\n",
       "      <td>Die Universitäten stehen vor einem sehr schwie...</td>\n",
       "      <td>Norbert Wallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5553</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>1095 words</td>\n",
       "      <td>Der Zielstrebige; Der neue Neubiberger Bürgerm...</td>\n",
       "      <td>Neubiberg - Zum Treffen im Zukunftswald im Lan...</td>\n",
       "      <td>VON DANIELA BODE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6039</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>441 words</td>\n",
       "      <td>Zwei Millionen für die Gemeinde; Altomünster w...</td>\n",
       "      <td>Altomünster - Die Marktgemeinde wird im Sommer...</td>\n",
       "      <td>KRAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6237</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>619 words</td>\n",
       "      <td>MEIN JOB; Feld statt Freizeit; Hotel-Azubi Flo...</td>\n",
       "      <td>,,Eigentlich mache ich momentan eine Ausbildun...</td>\n",
       "      <td>PROTOKOLL: NINA BÜCHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2162</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>1187 words</td>\n",
       "      <td>So geht es in den Kitas weiter; Seit Mitte Mär...</td>\n",
       "      <td>marlen Kess Düsseldorf Seit sieben Wochen sind...</td>\n",
       "      <td>Keß, Marlen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>263</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>531 words</td>\n",
       "      <td>Der Brainergy-Park baut seinen Vorsprung aus; ...</td>\n",
       "      <td>Von Guido Jansen   Jülich Der Brainergy-Park h...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2976</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>334 words</td>\n",
       "      <td>600 Infektionen in Fleischindustrie; NRW will ...</td>\n",
       "      <td>Düsseldorf (dpa) In Deutschland gibt es derzei...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6992</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>1050 words</td>\n",
       "      <td>Und was macht Berlin? Die Hauptstadt hat sich ...</td>\n",
       "      <td>B erlin reiht sich nicht ein in die Riege der ...</td>\n",
       "      <td>Sabine Beikler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4327</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>528 words</td>\n",
       "      <td>Österreich sperrt wieder auf; Von 1. Mai an ge...</td>\n",
       "      <td>Wien - Die erste Öffnung hat keinen Rückschlag...</td>\n",
       "      <td>SZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5199</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>966 words</td>\n",
       "      <td>Stadtrat ermahnt die Demonstranten; 6563 Infek...</td>\n",
       "      <td>München - Haushaltskonsolidierung, Gesundheits...</td>\n",
       "      <td>VON DOMINIK HUTTERSZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4105</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>264 words</td>\n",
       "      <td>ÜB fragt nach Einsparmöglichkeiten</td>\n",
       "      <td>Dachau - Die Fraktion der Überparteilichen Bür...</td>\n",
       "      <td>SZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3193</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>339 words</td>\n",
       "      <td>Vier Milliarden Euro weniger in der Landeskasse</td>\n",
       "      <td>Genau beziffern kann Finanzministerin Edith Si...</td>\n",
       "      <td>Christiane Rebhan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>276</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>726 words</td>\n",
       "      <td>Bevölkerung fühlt sich stark durch Corona bela...</td>\n",
       "      <td>Freiburg Ein Wissenschaftlerteam der Universit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5517</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>1116 words</td>\n",
       "      <td>FORUM; Europa solidarisch denken; Die gemeinsa...</td>\n",
       "      <td>Unser Kontinent steht inmitten einer Bewährung...</td>\n",
       "      <td>Von Oliver Hermes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6225</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>277 words</td>\n",
       "      <td>4000 Firmen in Kurzarbeit; Zusätzlich verliere...</td>\n",
       "      <td>Landkreis - Wegen der Corona-Krise ist auch im...</td>\n",
       "      <td>SZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1918</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>455 words</td>\n",
       "      <td>CDU Rheurdt: Vorbereitung auf den Wahlkampf in...</td>\n",
       "      <td>Rheurdt (got) Im Nachhinein ist Robert Peerenb...</td>\n",
       "      <td>Gottschlich, Peter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3236</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>834 words</td>\n",
       "      <td>Steiniger Weg zum Schulstart; Verwaltung wollt...</td>\n",
       "      <td>Normalerweise würden Felix Winkler und seine K...</td>\n",
       "      <td>Barbara Czimmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>3677</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>383 words</td>\n",
       "      <td>Sanierung soll zügig erfolgen</td>\n",
       "      <td>Der Betriebsausschuss Städtische Gebäude hat z...</td>\n",
       "      <td>Andreas Pflüger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4010</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>953 words</td>\n",
       "      <td>Geliebte des Diktators; Mario Vargas Llosa erz...</td>\n",
       "      <td>Seit er als verschreckter Schüler einer Kadett...</td>\n",
       "      <td>VON RUDOLF VON BITTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2110</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>788 words</td>\n",
       "      <td>Caritas will Wohnungslosigkeit verhindern; Nur...</td>\n",
       "      <td>Peter Clement Haan/Kreis Mettmann In dieser St...</td>\n",
       "      <td>Clement, Peter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>514</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>205 words</td>\n",
       "      <td>Urlaub im Zeichen des Virus</td>\n",
       "      <td>Eine entspannte Urlaubsreise sieht sicher ande...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>33</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>480 words</td>\n",
       "      <td>Müller: 50 Milliarden für Entwicklungsländer; ...</td>\n",
       "      <td>Berlin Entwicklungsminister Gerd Müller plädie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3931</td>\n",
       "      <td>Süddeutsche Zeitung (inkl. Regionalausgaben)</td>\n",
       "      <td>2020-04-25</td>\n",
       "      <td>1215 words</td>\n",
       "      <td>100 Beatmungsgeräte für 200 Millionen Menschen...</td>\n",
       "      <td>Berlin/München - Knapp 250 Menschen sind in So...</td>\n",
       "      <td>KRISTIANA LUDWIG, ANNA REUSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6350</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>769 words</td>\n",
       "      <td>Vorbild Südkorea Der Petersberger Klimadialog ...</td>\n",
       "      <td>Berlin - Der Petersberger Klimadialog soll all...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>6657</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>260 words</td>\n",
       "      <td>Bundesbank: Die Rezession wird schwer Berater:...</td>\n",
       "      <td>Frankfurt am Main/Brüssel - Die deutsche Wirts...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7092</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>1227 words</td>\n",
       "      <td>Von Aids für Corona lernen Die Krise in den 19...</td>\n",
       "      <td>Die Coronakrise sei die schlimmste Pandemie se...</td>\n",
       "      <td>Dirk Ludigs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2060</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>629 words</td>\n",
       "      <td>Statt Hygiene-Demo nur Kundgebung; In Dinsla...</td>\n",
       "      <td>Dinslaken (aha)Tausende haben in den vergangen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1014</td>\n",
       "      <td>Die Welt</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>839 words</td>\n",
       "      <td>Europas Superkampfjet soll kein Killerroboter ...</td>\n",
       "      <td>Wie sehen in 20, 30, 40 oder 50 Jahren Krieg u...</td>\n",
       "      <td>Gerhard Hegmann</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                     Newspaper        Date  \\\n",
       "0   2471                               Rheinische Post  2020-05-02   \n",
       "1   6803                              Der Tagesspiegel  2020-05-19   \n",
       "2    779                                      Die Welt  2020-04-22   \n",
       "3   3110                           Stuttgarter Zeitung  2020-04-30   \n",
       "4   3266                           Stuttgarter Zeitung  2020-04-25   \n",
       "5    738                                      Die Welt  2020-04-22   \n",
       "6   1122                                      Die Welt  2020-05-04   \n",
       "7   5298  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-14   \n",
       "8   4761  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-08   \n",
       "9   1538                               Rheinische Post  2020-04-23   \n",
       "10  5045  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-15   \n",
       "11   573                              Aachener Zeitung  2020-05-02   \n",
       "12  5591  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-02   \n",
       "13  5364  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-12   \n",
       "14   815                                      Die Welt  2020-04-20   \n",
       "15  3401                           Stuttgarter Zeitung  2020-05-07   \n",
       "16  2192                               Rheinische Post  2020-05-13   \n",
       "17  5156  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-16   \n",
       "18  4437  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-21   \n",
       "19  5907  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-05   \n",
       "20  4917  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-18   \n",
       "21  2045                               Rheinische Post  2020-05-15   \n",
       "22  5690  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-08   \n",
       "23  6878                              Der Tagesspiegel  2020-05-15   \n",
       "24  1842                               Rheinische Post  2020-04-22   \n",
       "25  4863  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-11   \n",
       "26  1607                               Rheinische Post  2020-04-24   \n",
       "27  3337                           Stuttgarter Zeitung  2020-04-27   \n",
       "28  5843  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-06   \n",
       "29  4338  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-28   \n",
       "..   ...                                           ...         ...   \n",
       "68  4513  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-29   \n",
       "69  2691                               Rheinische Post  2020-05-07   \n",
       "70  3082                           Stuttgarter Zeitung  2020-04-29   \n",
       "71  5553  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-05   \n",
       "72  6039  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-04   \n",
       "73  6237  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-02   \n",
       "74  2162                               Rheinische Post  2020-05-14   \n",
       "75   263                              Aachener Zeitung  2020-05-07   \n",
       "76  2976                               Rheinische Post  2020-05-14   \n",
       "77  6992                              Der Tagesspiegel  2020-05-07   \n",
       "78  4327  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-29   \n",
       "79  5199  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-14   \n",
       "80  4105  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-27   \n",
       "81  3193                           Stuttgarter Zeitung  2020-04-29   \n",
       "82   276                              Aachener Zeitung  2020-05-19   \n",
       "83  5517  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-11   \n",
       "84  6225  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-05-02   \n",
       "85  1918                               Rheinische Post  2020-05-04   \n",
       "86  3236                           Stuttgarter Zeitung  2020-04-30   \n",
       "87  3677                           Stuttgarter Zeitung  2020-05-15   \n",
       "88  4010  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-22   \n",
       "89  2110                               Rheinische Post  2020-05-15   \n",
       "90   514                              Aachener Zeitung  2020-05-14   \n",
       "91    33                              Aachener Zeitung  2020-04-20   \n",
       "92  3931  Süddeutsche Zeitung (inkl. Regionalausgaben)  2020-04-25   \n",
       "93  6350                              Der Tagesspiegel  2020-04-27   \n",
       "94  6657                              Der Tagesspiegel  2020-04-21   \n",
       "95  7092                              Der Tagesspiegel  2020-05-11   \n",
       "96  2060                               Rheinische Post  2020-05-14   \n",
       "97  1014                                      Die Welt  2020-05-18   \n",
       "\n",
       "        Length                                           Headline  \\\n",
       "0    291 words                     Die Demontage einer Ministerin   \n",
       "1    418 words  Microsoft mahnt Berlin ab Der US-Konzern verla...   \n",
       "2    595 words  Bolsonaro verliert die Nerven; Brasiliens Staa...   \n",
       "3    287 words                         Anklage im Mordfall Lübcke   \n",
       "4    198 words  Maskenpflicht auchin Schulen?; Bundesbildungsm...   \n",
       "5    931 words  Einsame Spitze; Das Aus der SAP-Chefin Morgan ...   \n",
       "6    872 words                 EU-Solidarität, von allen für alle   \n",
       "7    611 words  Parkplatz-Kneipen erlaubt; Stadt kommt Wirten ...   \n",
       "8    705 words  Bürgerrechte gelten auch im Notstand; Grüne be...   \n",
       "9    652 words  CDU fordert Rückbau von zwei Umweltspuren; Tes...   \n",
       "10   427 words  Aiwanger plante Notunterkunft; Wirtschaftsmini...   \n",
       "11    92 words         Volksentscheid über Cannabis-Legalisierung   \n",
       "12   734 words  Die Kontrolle über den Text; Der BGH fällt zwe...   \n",
       "13   390 words  Es gibt nix Feiners wia a Schweiners!; Bayernp...   \n",
       "14  1255 words  \"Mit künstlicher Beatmung wird viel Geld gemac...   \n",
       "15  1084 words         Eine Friseurin wird zur Freiheitskämpferin   \n",
       "16   318 words               Zwei Beutelspender für den Burgacker   \n",
       "17   513 words  Kür nach Kampfkandidatur; Stefan Ziegler führt...   \n",
       "18   312 words  ,,Vollkommener Schmarrn'; Kirchseeon setzt Ger...   \n",
       "19   841 words  Einsamer Abschied; Jakob Hartl hat am Donnerst...   \n",
       "20   310 words  Schaden für die Meinungsfreiheit; 700 afrikani...   \n",
       "21   169 words                           Nicht mit zweierlei Maß!   \n",
       "22   577 words                   EUROPA; Die Öffnung ist zwingend   \n",
       "23   858 words  Verlängerter Arm der Regierung Die Reform des ...   \n",
       "24   278 words  Stadt verzichtet auf Terrassengebühr; Gastwirt...   \n",
       "25   754 words  Alte Garde, aber nicht von gestern; Monika Sch...   \n",
       "26    60 words          CDU will mehr Sicherheit für den Römerweg   \n",
       "27   342 words                       Rauchen und Grillen verboten   \n",
       "28   825 words  Party, aber bitte mit Maske; In New York wächs...   \n",
       "29   239 words  ,,Kümmerer' für Moosburg; CSU-Fraktion fordert...   \n",
       "..         ...                                                ...   \n",
       "68   507 words  Aslyunterkunft unter Quarantäne; Sicherheitsdi...   \n",
       "69   109 words  Bundesregierung: Balearen-Urlaub nicht abschr...   \n",
       "70   499 words         Sind Unis digital zu schlecht aufgestellt?   \n",
       "71  1095 words  Der Zielstrebige; Der neue Neubiberger Bürgerm...   \n",
       "72   441 words  Zwei Millionen für die Gemeinde; Altomünster w...   \n",
       "73   619 words  MEIN JOB; Feld statt Freizeit; Hotel-Azubi Flo...   \n",
       "74  1187 words  So geht es in den Kitas weiter; Seit Mitte Mär...   \n",
       "75   531 words  Der Brainergy-Park baut seinen Vorsprung aus; ...   \n",
       "76   334 words  600 Infektionen in Fleischindustrie; NRW will ...   \n",
       "77  1050 words  Und was macht Berlin? Die Hauptstadt hat sich ...   \n",
       "78   528 words  Österreich sperrt wieder auf; Von 1. Mai an ge...   \n",
       "79   966 words  Stadtrat ermahnt die Demonstranten; 6563 Infek...   \n",
       "80   264 words                 ÜB fragt nach Einsparmöglichkeiten   \n",
       "81   339 words    Vier Milliarden Euro weniger in der Landeskasse   \n",
       "82   726 words  Bevölkerung fühlt sich stark durch Corona bela...   \n",
       "83  1116 words  FORUM; Europa solidarisch denken; Die gemeinsa...   \n",
       "84   277 words  4000 Firmen in Kurzarbeit; Zusätzlich verliere...   \n",
       "85   455 words  CDU Rheurdt: Vorbereitung auf den Wahlkampf in...   \n",
       "86   834 words  Steiniger Weg zum Schulstart; Verwaltung wollt...   \n",
       "87   383 words                      Sanierung soll zügig erfolgen   \n",
       "88   953 words  Geliebte des Diktators; Mario Vargas Llosa erz...   \n",
       "89   788 words  Caritas will Wohnungslosigkeit verhindern; Nur...   \n",
       "90   205 words                        Urlaub im Zeichen des Virus   \n",
       "91   480 words  Müller: 50 Milliarden für Entwicklungsländer; ...   \n",
       "92  1215 words  100 Beatmungsgeräte für 200 Millionen Menschen...   \n",
       "93   769 words  Vorbild Südkorea Der Petersberger Klimadialog ...   \n",
       "94   260 words  Bundesbank: Die Rezession wird schwer Berater:...   \n",
       "95  1227 words  Von Aids für Corona lernen Die Krise in den 19...   \n",
       "96   629 words  Statt Hygiene-Demo nur Kundgebung; In Dinsla...   \n",
       "97   839 words  Europas Superkampfjet soll kein Killerroboter ...   \n",
       "\n",
       "                                              Article  \\\n",
       "0   Maximilian Plück Armin Laschet brauchte am Don...   \n",
       "1   Gegenwind für die Berliner Beauftragte für Dat...   \n",
       "2   Jair Bolsonaro sieht müde aus, mehrfach hustet...   \n",
       "3   Die Bundesanwaltschaft hat Anklage im Fall des...   \n",
       "4   Bundesbildungsministerin Anja Karliczek (CDU) ...   \n",
       "5   Doppelspitzen erfüllen nur selten ihren Zweck,...   \n",
       "6   Statt gemeinsam die Seuche zu bekämpfen, wurde...   \n",
       "7   Ein Bier, wo bisher das Auto stand? Schnitzel ...   \n",
       "8   Landkreis - Die Zuwächse der Grünen sind zulet...   \n",
       "9   Hendrik Gaasterland Düsseldorf Die CDU-Fraktio...   \n",
       "10  München - Wirtschaftsminister Hubert Aiwanger ...   \n",
       "11  Wellington Neuseeland entscheidet im September...   \n",
       "12  Als die Westdeutsche Allgemeine Zeitung (WAZ) ...   \n",
       "13  Grafing/Ebersberg - Nach dem Motto ,,Gleiches ...   \n",
       "14  Jürgen Wasem ist einer der bekanntesten Gesund...   \n",
       "15  Wenn wir ans Telefon gehen, haben wir auch geö...   \n",
       "16  Viersen (tre) Mit zwei Kotbeutelspendern möcht...   \n",
       "17  Trudering/Riem - Mit gebündelten Kräften haben...   \n",
       "18  Kirchseeon - Kindergärten und Tagesstätten sin...   \n",
       "19  Nandlstadt - Jakob Hartl hat am Donnerstag nac...   \n",
       "20  In der Kontroverse um den afrikanischen Histor...   \n",
       "21  Man kann einerseits verstehen, warum Friedrich...   \n",
       "22  Als die Not immer größer wurde, fiel den EU-St...   \n",
       "23  Es sind Stücke aus dem Tollhaus. Der neue Mite...   \n",
       "24  Viersen (mrö) Die Stadt Viersen wird für den Z...   \n",
       "25  Planegg - Zwölf von 24 Gemeinderäten - ohne de...   \n",
       "26  Erkrath (hup) Die CDU setzt sich für einen bes...   \n",
       "27  Das schöne Wetter lockt in diesen Tagen viele ...   \n",
       "28  New York - Wenn man nach New York City blickt,...   \n",
       "29  Moosburg - Die ,,aktive Unterstützung von Moos...   \n",
       "..                                                ...   \n",
       "68  Erding - Die Forsterner Grünen kritisieren in ...   \n",
       "69  Berlin (dpa) Urlauber können im Sommer unter U...   \n",
       "70  Die Universitäten stehen vor einem sehr schwie...   \n",
       "71  Neubiberg - Zum Treffen im Zukunftswald im Lan...   \n",
       "72  Altomünster - Die Marktgemeinde wird im Sommer...   \n",
       "73  ,,Eigentlich mache ich momentan eine Ausbildun...   \n",
       "74  marlen Kess Düsseldorf Seit sieben Wochen sind...   \n",
       "75  Von Guido Jansen   Jülich Der Brainergy-Park h...   \n",
       "76  Düsseldorf (dpa) In Deutschland gibt es derzei...   \n",
       "77  B erlin reiht sich nicht ein in die Riege der ...   \n",
       "78  Wien - Die erste Öffnung hat keinen Rückschlag...   \n",
       "79  München - Haushaltskonsolidierung, Gesundheits...   \n",
       "80  Dachau - Die Fraktion der Überparteilichen Bür...   \n",
       "81  Genau beziffern kann Finanzministerin Edith Si...   \n",
       "82  Freiburg Ein Wissenschaftlerteam der Universit...   \n",
       "83  Unser Kontinent steht inmitten einer Bewährung...   \n",
       "84  Landkreis - Wegen der Corona-Krise ist auch im...   \n",
       "85  Rheurdt (got) Im Nachhinein ist Robert Peerenb...   \n",
       "86  Normalerweise würden Felix Winkler und seine K...   \n",
       "87  Der Betriebsausschuss Städtische Gebäude hat z...   \n",
       "88  Seit er als verschreckter Schüler einer Kadett...   \n",
       "89  Peter Clement Haan/Kreis Mettmann In dieser St...   \n",
       "90  Eine entspannte Urlaubsreise sieht sicher ande...   \n",
       "91  Berlin Entwicklungsminister Gerd Müller plädie...   \n",
       "92  Berlin/München - Knapp 250 Menschen sind in So...   \n",
       "93  Berlin - Der Petersberger Klimadialog soll all...   \n",
       "94  Frankfurt am Main/Brüssel - Die deutsche Wirts...   \n",
       "95  Die Coronakrise sei die schlimmste Pandemie se...   \n",
       "96  Dinslaken (aha)Tausende haben in den vergangen...   \n",
       "97  Wie sehen in 20, 30, 40 oder 50 Jahren Krieg u...   \n",
       "\n",
       "                          Author  \n",
       "0              Plück, Maximilian  \n",
       "1                            NaN  \n",
       "2                  Tobias Käufer  \n",
       "3                            AFP  \n",
       "4                            dpa  \n",
       "5                    Klaus Boldt  \n",
       "6                Jürgen Rüttgers  \n",
       "7                            NaN  \n",
       "8                  IRIS HILBERTH  \n",
       "9           Gaasterland, Hendrik  \n",
       "10                  LISA SCHNELL  \n",
       "11                           NaN  \n",
       "12              WOLFGANG JANISCH  \n",
       "13                           WKB  \n",
       "14                   Kaja Klapsa  \n",
       "15                Frank Herrmann  \n",
       "16               Treffer, Bianca  \n",
       "17                  ILONA GERDOM  \n",
       "18                           AJU  \n",
       "19              KATHARINA AURICH  \n",
       "20                   SONJA ZEKRI  \n",
       "21                           NaN  \n",
       "22            VON JENS SCHNEIDER  \n",
       "23               Reinhart Bünger  \n",
       "24                  Röse, Martin  \n",
       "25    VON RAINER RUTZRAINER RUTZ  \n",
       "26                           NaN  \n",
       "27                           wed  \n",
       "28             CHRISTIAN ZASCHKE  \n",
       "29                          AXKA  \n",
       "..                           ...  \n",
       "68                      FLO, TDR  \n",
       "69                           NaN  \n",
       "70                Norbert Wallet  \n",
       "71              VON DANIELA BODE  \n",
       "72                          KRAM  \n",
       "73         PROTOKOLL: NINA BÜCHS  \n",
       "74                   Keß, Marlen  \n",
       "75                           NaN  \n",
       "76                           NaN  \n",
       "77                Sabine Beikler  \n",
       "78                            SZ  \n",
       "79          VON DOMINIK HUTTERSZ  \n",
       "80                            SZ  \n",
       "81             Christiane Rebhan  \n",
       "82                           NaN  \n",
       "83             Von Oliver Hermes  \n",
       "84                            SZ  \n",
       "85            Gottschlich, Peter  \n",
       "86               Barbara Czimmer  \n",
       "87               Andreas Pflüger  \n",
       "88         VON RUDOLF VON BITTER  \n",
       "89                Clement, Peter  \n",
       "90                           NaN  \n",
       "91                           NaN  \n",
       "92  KRISTIANA LUDWIG, ANNA REUSS  \n",
       "93                           NaN  \n",
       "94                           NaN  \n",
       "95                   Dirk Ludigs  \n",
       "96                           NaN  \n",
       "97               Gerhard Hegmann  \n",
       "\n",
       "[348 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CID</th>\n",
       "      <th>AID</th>\n",
       "      <th>NO</th>\n",
       "      <th>CPA</th>\n",
       "      <th>BOV</th>\n",
       "      <th>BOA</th>\n",
       "      <th>NEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>03</td>\n",
       "      <td>3414</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>03</td>\n",
       "      <td>6996</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>03</td>\n",
       "      <td>4894</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>03</td>\n",
       "      <td>3110</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index CID   AID NO CPA BOV BOA NEU\n",
       "0      1  03   490  1   3   2   2   2\n",
       "1      2  03  3414  3   1   1   1   2\n",
       "2      3  03  6996  4   3   2   2   1\n",
       "3      4  03  4894  5   3   2   2   1\n",
       "4      5  03  3110  3   3   2   2   2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting only the relevant columns\n",
    "mca = mca[[\"CID\", \"AID\", \"NO\", \"CPA\", \"BOV\", \"BOA\", \"NEU\"]]\n",
    "#drop the first row, because it contains the column names\n",
    "mca = mca.drop(mca.index[0])\n",
    "#reset the index\n",
    "mca = mca.reset_index()\n",
    "#inspect the data\n",
    "mca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude rows with empty values\n",
    "mca = mca[mca[\"BOV\"].notna()]\n",
    "mca = mca[mca[\"AID\"].notna()]\n",
    "mca = mca[mca[\"CID\"].notna()]\n",
    "mca = mca[mca[\"NEU\"].notna()]\n",
    "#change the datatypes of the manually coded df\n",
    "mca[\"AID\"] = mca[\"AID\"].astype(int)\n",
    "mca[\"CID\"] = mca[\"CID\"].astype(int)\n",
    "mca[\"NO\"] = mca[\"NO\"].astype(int)\n",
    "mca[\"CPA\"] = mca[\"CPA\"].astype(int)\n",
    "mca[\"BOV\"] = mca[\"BOV\"].astype(int)\n",
    "mca[\"BOA\"] = mca[\"BOA\"].astype(int)\n",
    "mca[\"NEU\"] = mca[\"NEU\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AID</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Date</th>\n",
       "      <th>Length</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2471</td>\n",
       "      <td>Rheinische Post</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>291 words</td>\n",
       "      <td>Die Demontage einer Ministerin</td>\n",
       "      <td>Maximilian Plück Armin Laschet brauchte am Don...</td>\n",
       "      <td>Plück, Maximilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6803</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>418 words</td>\n",
       "      <td>Microsoft mahnt Berlin ab Der US-Konzern verla...</td>\n",
       "      <td>Gegenwind für die Berliner Beauftragte für Dat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>779</td>\n",
       "      <td>Die Welt</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>595 words</td>\n",
       "      <td>Bolsonaro verliert die Nerven; Brasiliens Staa...</td>\n",
       "      <td>Jair Bolsonaro sieht müde aus, mehrfach hustet...</td>\n",
       "      <td>Tobias Käufer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AID         Newspaper        Date     Length  \\\n",
       "0  2471   Rheinische Post  2020-05-02  291 words   \n",
       "1  6803  Der Tagesspiegel  2020-05-19  418 words   \n",
       "2   779          Die Welt  2020-04-22  595 words   \n",
       "\n",
       "                                            Headline  \\\n",
       "0                     Die Demontage einer Ministerin   \n",
       "1  Microsoft mahnt Berlin ab Der US-Konzern verla...   \n",
       "2  Bolsonaro verliert die Nerven; Brasiliens Staa...   \n",
       "\n",
       "                                             Article             Author  \n",
       "0  Maximilian Plück Armin Laschet brauchte am Don...  Plück, Maximilian  \n",
       "1  Gegenwind für die Berliner Beauftragte für Dat...                NaN  \n",
       "2  Jair Bolsonaro sieht müde aus, mehrfach hustet...      Tobias Käufer  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = articles.rename(columns= {\"ID\":\"AID\"})\n",
    "articles.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the AID column dtype for the article df -> necessary for merging\n",
    "articles[\"AID\"] = articles[\"AID\"].astype(int)\n",
    "#merge the mca data with the article data\n",
    "df = mca.merge(articles, how=\"left\", on=\"AID\")\n",
    "#check for duplicates\n",
    "duplicates = df[df.duplicated([\"AID\"])]\n",
    "#check the length\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CID</th>\n",
       "      <th>AID</th>\n",
       "      <th>NO</th>\n",
       "      <th>CPA</th>\n",
       "      <th>BOV</th>\n",
       "      <th>BOA</th>\n",
       "      <th>NEU</th>\n",
       "      <th>Newspaper_x</th>\n",
       "      <th>Date</th>\n",
       "      <th>Length</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article_x</th>\n",
       "      <th>Author</th>\n",
       "      <th>Article_y</th>\n",
       "      <th>Newspaper_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>43 words</td>\n",
       "      <td>FDP lädt ein zur Wahlversammlung</td>\n",
       "      <td>Simmerath Der FDP Ortsverein Simmerath lädt zu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3414</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>576 words</td>\n",
       "      <td>Wer stopft das Steuerloch?</td>\n",
       "      <td>Angesichts der gigantischen Steuer-ausfälle im...</td>\n",
       "      <td>Thorsten Knuf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6996</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>801 words</td>\n",
       "      <td>Schulbetrieb und Klassenfahrt</td>\n",
       "      <td>\"Kindeswohlgefährdung begünstigt. Experten bes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  CID   AID  NO  CPA  BOV  BOA  NEU          Newspaper_x        Date  \\\n",
       "0      1    3   490   1    3    2    2    2     Aachener Zeitung  2020-05-15   \n",
       "1      2    3  3414   3    1    1    1    2  Stuttgarter Zeitung  2020-05-15   \n",
       "2      3    3  6996   4    3    2    2    1     Der Tagesspiegel  2020-05-10   \n",
       "\n",
       "      Length                          Headline  \\\n",
       "0   43 words  FDP lädt ein zur Wahlversammlung   \n",
       "1  576 words        Wer stopft das Steuerloch?   \n",
       "2  801 words     Schulbetrieb und Klassenfahrt   \n",
       "\n",
       "                                           Article_x         Author Article_y  \\\n",
       "0  Simmerath Der FDP Ortsverein Simmerath lädt zu...            NaN       NaN   \n",
       "1  Angesichts der gigantischen Steuer-ausfälle im...  Thorsten Knuf       NaN   \n",
       "2  \"Kindeswohlgefährdung begünstigt. Experten bes...            NaN       NaN   \n",
       "\n",
       "  Newspaper_y  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns\n",
    "articles_online = articles_online.rename(columns={\"ID\":\"AID\"})\n",
    "#select only the AID (to merge on) and the text\n",
    "articles_online = articles_online[[\"AID\", \"Article\", \"Newspaper\"]]\n",
    "#merge the dataframe with the online article data\n",
    "df = df.merge(articles_online, how=\"left\", on=\"AID\")\n",
    "#inspect the data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CID</th>\n",
       "      <th>AID</th>\n",
       "      <th>NO</th>\n",
       "      <th>CPA</th>\n",
       "      <th>BOV</th>\n",
       "      <th>BOA</th>\n",
       "      <th>NEU</th>\n",
       "      <th>Newspaper_x</th>\n",
       "      <th>Date</th>\n",
       "      <th>Length</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article_x</th>\n",
       "      <th>Author</th>\n",
       "      <th>Article_y</th>\n",
       "      <th>Newspaper_y</th>\n",
       "      <th>Article</th>\n",
       "      <th>Newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>43 words</td>\n",
       "      <td>FDP lädt ein zur Wahlversammlung</td>\n",
       "      <td>Simmerath Der FDP Ortsverein Simmerath lädt zu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Simmerath Der FDP Ortsverein Simmerath lädt zu...</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3414</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>576 words</td>\n",
       "      <td>Wer stopft das Steuerloch?</td>\n",
       "      <td>Angesichts der gigantischen Steuer-ausfälle im...</td>\n",
       "      <td>Thorsten Knuf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Angesichts der gigantischen Steuer-ausfälle im...</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6996</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>801 words</td>\n",
       "      <td>Schulbetrieb und Klassenfahrt</td>\n",
       "      <td>\"Kindeswohlgefährdung begünstigt. Experten bes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\"Kindeswohlgefährdung begünstigt. Experten bes...</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  CID   AID  NO  CPA  BOV  BOA  NEU          Newspaper_x        Date  \\\n",
       "0      1    3   490   1    3    2    2    2     Aachener Zeitung  2020-05-15   \n",
       "1      2    3  3414   3    1    1    1    2  Stuttgarter Zeitung  2020-05-15   \n",
       "2      3    3  6996   4    3    2    2    1     Der Tagesspiegel  2020-05-10   \n",
       "\n",
       "      Length                          Headline  \\\n",
       "0   43 words  FDP lädt ein zur Wahlversammlung   \n",
       "1  576 words        Wer stopft das Steuerloch?   \n",
       "2  801 words     Schulbetrieb und Klassenfahrt   \n",
       "\n",
       "                                           Article_x         Author Article_y  \\\n",
       "0  Simmerath Der FDP Ortsverein Simmerath lädt zu...            NaN             \n",
       "1  Angesichts der gigantischen Steuer-ausfälle im...  Thorsten Knuf             \n",
       "2  \"Kindeswohlgefährdung begünstigt. Experten bes...            NaN             \n",
       "\n",
       "  Newspaper_y                                            Article  \\\n",
       "0              Simmerath Der FDP Ortsverein Simmerath lädt zu...   \n",
       "1              Angesichts der gigantischen Steuer-ausfälle im...   \n",
       "2              \"Kindeswohlgefährdung begünstigt. Experten bes...   \n",
       "\n",
       "             Newspaper  \n",
       "0     Aachener Zeitung  \n",
       "1  Stuttgarter Zeitung  \n",
       "2     Der Tagesspiegel  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fill empty columns\n",
    "df[\"Article_x\"]= df[\"Article_x\"].fillna(\"\")\n",
    "df[\"Article_y\"]= df[\"Article_y\"].fillna(\"\") \n",
    "#create a unified text column\n",
    "df[\"Article\"] = df[\"Article_x\"] + df[\"Article_y\"]\n",
    "\n",
    "#fill empty columns\n",
    "df[\"Newspaper_x\"]= df[\"Newspaper_x\"].fillna(\"\")\n",
    "df[\"Newspaper_y\"]= df[\"Newspaper_y\"].fillna(\"\") \n",
    "#create a unified text column\n",
    "df[\"Newspaper\"] = df[\"Newspaper_x\"] + df[\"Newspaper_y\"]\n",
    "\n",
    "#inspect df\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Article\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplicates\n",
    "df.drop_duplicates(subset =\"AID\", keep = \"first\", inplace = True) \n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see if there is missing data\n",
    "df = df[df[\"Article\"].notna()]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove codes that don't align with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Newspaper'] != \"\"]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CID</th>\n",
       "      <th>AID</th>\n",
       "      <th>NO</th>\n",
       "      <th>CPA</th>\n",
       "      <th>BOV</th>\n",
       "      <th>BOA</th>\n",
       "      <th>NEU</th>\n",
       "      <th>Newspaper_x</th>\n",
       "      <th>Date</th>\n",
       "      <th>Length</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article_x</th>\n",
       "      <th>Author</th>\n",
       "      <th>Article_y</th>\n",
       "      <th>Newspaper_y</th>\n",
       "      <th>Article</th>\n",
       "      <th>Newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>43 words</td>\n",
       "      <td>FDP lädt ein zur Wahlversammlung</td>\n",
       "      <td>Simmerath Der FDP Ortsverein Simmerath lädt zu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Simmerath Der FDP Ortsverein Simmerath lädt zu...</td>\n",
       "      <td>Aachener Zeitung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3414</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>576 words</td>\n",
       "      <td>Wer stopft das Steuerloch?</td>\n",
       "      <td>Angesichts der gigantischen Steuer-ausfälle im...</td>\n",
       "      <td>Thorsten Knuf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Angesichts der gigantischen Steuer-ausfälle im...</td>\n",
       "      <td>Stuttgarter Zeitung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6996</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>801 words</td>\n",
       "      <td>Schulbetrieb und Klassenfahrt</td>\n",
       "      <td>\"Kindeswohlgefährdung begünstigt. Experten bes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\"Kindeswohlgefährdung begünstigt. Experten bes...</td>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  CID   AID  NO  CPA  BOV  BOA  NEU          Newspaper_x        Date  \\\n",
       "0      1    3   490   1    3    0    0    1     Aachener Zeitung  2020-05-15   \n",
       "1      2    3  3414   3    1    1    1    1  Stuttgarter Zeitung  2020-05-15   \n",
       "2      3    3  6996   4    3    0    0    0     Der Tagesspiegel  2020-05-10   \n",
       "\n",
       "      Length                          Headline  \\\n",
       "0   43 words  FDP lädt ein zur Wahlversammlung   \n",
       "1  576 words        Wer stopft das Steuerloch?   \n",
       "2  801 words     Schulbetrieb und Klassenfahrt   \n",
       "\n",
       "                                           Article_x         Author Article_y  \\\n",
       "0  Simmerath Der FDP Ortsverein Simmerath lädt zu...            NaN             \n",
       "1  Angesichts der gigantischen Steuer-ausfälle im...  Thorsten Knuf             \n",
       "2  \"Kindeswohlgefährdung begünstigt. Experten bes...            NaN             \n",
       "\n",
       "  Newspaper_y                                            Article  \\\n",
       "0              Simmerath Der FDP Ortsverein Simmerath lädt zu...   \n",
       "1              Angesichts der gigantischen Steuer-ausfälle im...   \n",
       "2              \"Kindeswohlgefährdung begünstigt. Experten bes...   \n",
       "\n",
       "             Newspaper  \n",
       "0     Aachener Zeitung  \n",
       "1  Stuttgarter Zeitung  \n",
       "2     Der Tagesspiegel  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NEU\"] = df[\"NEU\"].replace([2, 1], [1, 0])\n",
    "df[\"BOV\"] = df[\"BOA\"].replace(2, 0)\n",
    "df[\"BOA\"] = df[\"BOA\"].replace(2, 0)\n",
    "#inspect data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('german')) \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "from collections import Counter\n",
    "\n",
    "#function to remove stopwords\n",
    "def remove_stopwords_and_stem(text):\n",
    "    #apply SpaCy to tokenise\n",
    "    doc = nlp(text)\n",
    "    #create a list of tokens\n",
    "    tokens = [token.text for token in doc]\n",
    "    #create an empty list for all words in a text that are not stopwords\n",
    "    no_stopwords = []\n",
    "    #loop over the tokens list and append non-stopwords to the no_stopwords list\n",
    "    for w in tokens: \n",
    "        if w not in stop_words: \n",
    "            no_stopwords.append(w)\n",
    "    #stem all words in the list\n",
    "    stems=\"\"\n",
    "    for word in no_stopwords:\n",
    "        stems=stems + stemmer.stem(word) + \" \"\n",
    "    #return the final stems as a single string\n",
    "    return(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... create a column with the cleaned article text (where stopwords are removed and all words are stemmed)\n",
    "df[\"clean text\"] = [remove_stopwords_and_stem(text) for text in df[\"Article\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "#clean the text with \n",
    "def fixtext(string):\n",
    "    return ftfy.fix_text(string)\n",
    "df[\"Article\"] = df[\"Article\"].apply(fixtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into test and training set\n",
    "I reran the following models with both the original and the cleaned text. I indicated which one led to better results for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    283\n",
       "0    204\n",
       "Name: NEU, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NEU\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training and testing dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"Article\"], df.NEU, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "   \n",
    "I used three different types of text representations for the classifier training, namely count vectors, TF-IDF vectors, and TF-IDF vectors with n-grams. All classifiers were trained and tested on all three features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors\n",
    "In the following, Term Frequency-Inverse Document Frequency is applied in order to represent the text data as a vector that can be used as numerical input for a SML algorithm.   \n",
    "   \n",
    "I chose the following parameters, which are specified in the code below:\n",
    "- The text is represented as unigrams and bigrams, meaning not just single words, but also sets of two neighboring words are accounted for -> ngram_range\n",
    "- Terms that appear in less than 10 documents are ignored -> min_df\n",
    "- All other terms are included -> max_df\n",
    "- In total, up to 200 features can be extracted per text -> max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389, 200)\n",
      "(98, 200)\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=200)\n",
    "\n",
    "#features (=y_train)\n",
    "features_train_tfidf = tfidf_vect.fit_transform(x_train).toarray()\n",
    "labels_train_tfidf = y_train\n",
    "print(features_train_tfidf.shape)\n",
    "\n",
    "#features (=y_train)\n",
    "features_test_tfidf = tfidf_vect.fit_transform(x_test).toarray()\n",
    "labels_test_tfidf = y_test\n",
    "print(features_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389, 200)\n",
      "(98, 200)\n"
     ]
    }
   ],
   "source": [
    "# Parameter selection\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 200\n",
    "\n",
    "#Defining the TfidfVectorizer with the parameters above\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None, #already removed\n",
    "                        lowercase=False, #already done\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "#features (=x_train)\n",
    "features_train_tfidf_ngrams = tfidf.fit_transform(x_train).toarray()\n",
    "#labels (=x_test)\n",
    "labels_train_tfidf_ngrams = y_train\n",
    "print(features_train_tfidf_ngrams.shape)\n",
    "\n",
    "#features (=y_train)\n",
    "features_test_tfidf_ngrams = tfidf.transform(x_test).toarray()\n",
    "#labels (=y_test)\n",
    "labels_test_tfidf_ngrams = y_test\n",
    "print(features_test_tfidf_ngrams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors\n",
    "   \n",
    "transform the training and testing data using count vectorizer object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389, 200)\n",
      "(98, 200)\n"
     ]
    }
   ],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer=\"word\", token_pattern=r\"\\w{1,}\", max_features= 200)\n",
    "\n",
    "#features (=y_train)\n",
    "features_train_count = count_vect.fit_transform(x_train).toarray()\n",
    "labels_train_count = y_train\n",
    "print(features_train_count.shape)\n",
    "\n",
    "#features (=y_train)\n",
    "features_test_count = count_vect.fit_transform(x_test).toarray()\n",
    "labels_test_count = y_test\n",
    "print(features_test_count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SML: Naive Bayes Classifier\n",
    "\n",
    "*Important:* The naive bayes classifier achieves better results with the cleaned article text than with the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the models\n",
    "nbc_count_gnb = GaussianNB()\n",
    "nbc_tfidf_gnb = GaussianNB()\n",
    "nbc_tfidf_ngrams_gnb = GaussianNB()\n",
    "\n",
    "nbc_count_mnb = MultinomialNB()\n",
    "nbc_tfidf_mnb = MultinomialNB()\n",
    "nbc_tfidf_ngrams_mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifiers\n",
    "nbc_count_gnb.fit(features_train_count, labels_train_count)\n",
    "nbc_tfidf_gnb.fit(features_train_tfidf, labels_train_tfidf)\n",
    "nbc_tfidf_gnb.fit(features_train_tfidf, labels_train_tfidf_ngrams)\n",
    "nbc_count_mnb.fit(features_train_count, labels_train_count)\n",
    "nbc_tfidf_mnb.fit(features_train_tfidf, labels_train_tfidf)\n",
    "nbc_tfidf_mnb.fit(features_train_tfidf, labels_train_tfidf_ngrams)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_count_gnb = nbc_count_gnb.predict(features_test_count)\n",
    "predictions_tfidf_gnb = nbc_tfidf_gnb.predict(features_test_tfidf)\n",
    "predictions_tfidf_ngrams_gnb = nbc_tfidf_gnb.predict(features_test_tfidf_ngrams)\n",
    "predictions_count_mnb = nbc_count_mnb.predict(features_test_count)\n",
    "predictions_tfidf_mnb = nbc_tfidf_mnb.predict(features_test_tfidf)\n",
    "predictions_tfidf_ngrams_mnb = nbc_tfidf_mnb.predict(features_test_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assess accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - naive bayes classifier - GNB - count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64        42\n",
      "           1       0.74      0.57      0.65        56\n",
      "\n",
      "    accuracy                           0.64        98\n",
      "   macro avg       0.65      0.65      0.64        98\n",
      "weighted avg       0.67      0.64      0.64        98\n",
      "\n",
      "\n",
      "classification report - naive bayes classifier - GNB - tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.55      0.47        42\n",
      "           1       0.56      0.43      0.48        56\n",
      "\n",
      "    accuracy                           0.48        98\n",
      "   macro avg       0.49      0.49      0.48        98\n",
      "weighted avg       0.50      0.48      0.48        98\n",
      "\n",
      "\n",
      "classification report - naive bayes classifier - GNB - tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.76      0.62        42\n",
      "           1       0.73      0.48      0.58        56\n",
      "\n",
      "    accuracy                           0.60        98\n",
      "   macro avg       0.63      0.62      0.60        98\n",
      "weighted avg       0.64      0.60      0.60        98\n",
      "\n",
      "\n",
      "classification report - naive bayes classifier - MNB _count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49        42\n",
      "           1       0.61      0.59      0.60        56\n",
      "\n",
      "    accuracy                           0.55        98\n",
      "   macro avg       0.54      0.54      0.54        98\n",
      "weighted avg       0.55      0.55      0.55        98\n",
      "\n",
      "\n",
      "classification report - naive bayes classifier - MNB - tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.05      0.07        42\n",
      "           1       0.53      0.80      0.64        56\n",
      "\n",
      "    accuracy                           0.48        98\n",
      "   macro avg       0.34      0.43      0.36        98\n",
      "weighted avg       0.37      0.48      0.40        98\n",
      "\n",
      "\n",
      "classification report - naive bayes classifier - MNB - tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.38      0.47        42\n",
      "           1       0.64      0.82      0.72        56\n",
      "\n",
      "    accuracy                           0.63        98\n",
      "   macro avg       0.63      0.60      0.59        98\n",
      "weighted avg       0.63      0.63      0.61        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report - naive bayes classifier - GNB - count\") \n",
    "print(classification_report(labels_test_count,predictions_count_gnb))\n",
    "print(\"\")\n",
    "print(\"classification report - naive bayes classifier - GNB - tfidf\") \n",
    "print(classification_report(labels_test_count,predictions_tfidf_gnb))\n",
    "print(\"\")\n",
    "print(\"classification report - naive bayes classifier - GNB - tfidf\") \n",
    "print(classification_report(labels_test_count,predictions_tfidf_ngrams_gnb))\n",
    "print(\"\")\n",
    "print(\"classification report - naive bayes classifier - MNB _count\") \n",
    "print(classification_report(labels_test_count,predictions_count_mnb))\n",
    "print(\"\")\n",
    "print(\"classification report - naive bayes classifier - MNB - tfidf\") \n",
    "print(classification_report(labels_test_count,predictions_tfidf_mnb))\n",
    "print(\"\")\n",
    "print(\"classification report - naive bayes classifier - MNB - tfidf\") \n",
    "print(classification_report(labels_test_count,predictions_tfidf_ngrams_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the best model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes_classifier_results = predictions_count_gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SML: Support Vector Machines\n",
    "   \n",
    "*Important:* The support vector machines achieve better results with the complete article text than with the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'break_ties': False,\n",
      " 'cache_size': 200,\n",
      " 'class_weight': None,\n",
      " 'coef0': 0.0,\n",
      " 'decision_function_shape': 'ovr',\n",
      " 'degree': 3,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'probability': False,\n",
      " 'random_state': 8,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "#create a support vector machine (with random state for reproducibility)\n",
    "svc_0 =svm.SVC(random_state=8)\n",
    "#examine parameters\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(svc_0.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search Cross Validation   \n",
    "   \n",
    "#### Defining the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.0001, 0.001, 0.01],\n",
      " 'degree': [1, 2, 3, 4, 5],\n",
      " 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
      " 'kernel': ['linear', 'rbf', 'poly'],\n",
      " 'probability': [True]}\n"
     ]
    }
   ],
   "source": [
    "# C\n",
    "C = [.0001, .001, .01]\n",
    "\n",
    "# gamma\n",
    "gamma = [.0001, .001, .01, .1, 1, 10, 100]\n",
    "\n",
    "# degree\n",
    "degree = [1, 2, 3, 4, 5]\n",
    "\n",
    "# kernel\n",
    "kernel = ['linear', 'rbf', 'poly']\n",
    "\n",
    "# probability\n",
    "probability = [True]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C,\n",
    "              'kernel': kernel,\n",
    "              'gamma': gamma,\n",
    "              'degree': degree,\n",
    "              'probability': probability\n",
    "             }\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the random search models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "svc = svm.SVC(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=svc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "random_search_count = random_search\n",
    "random_search_tfidf = random_search\n",
    "random_search_tfidf_ngrams = random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit random search models and assess their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:   12.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search for count vectors are:\n",
      "{'probability': True, 'kernel': 'poly', 'gamma': 10, 'degree': 1, 'C': 0.0001}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.6915324985092427\n"
     ]
    }
   ],
   "source": [
    "# Fit the random search models\n",
    "random_search_count.fit(features_train_count, labels_train_count)\n",
    "\n",
    "print(\"The best hyperparameters from Random Search for count vectors are:\")\n",
    "print(random_search_count.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search_count.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search for tfidf vectors are:\n",
      "{'probability': True, 'kernel': 'poly', 'gamma': 10, 'degree': 4, 'C': 0.01}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.681315841780958\n"
     ]
    }
   ],
   "source": [
    "random_search_tfidf.fit(features_train_tfidf, labels_train_tfidf)\n",
    "\n",
    "print(\"The best hyperparameters from Random Search for tfidf vectors are:\")\n",
    "print(random_search_tfidf.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search for tfidf vectors with ngrams are:\n",
      "{'probability': True, 'kernel': 'poly', 'gamma': 10, 'degree': 4, 'C': 0.01}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.7276088252832439\n"
     ]
    }
   ],
   "source": [
    "random_search_tfidf_ngrams.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "\n",
    "print(\"The best hyperparameters from Random Search for tfidf vectors with ngrams are:\")\n",
    "print(random_search_tfidf_ngrams.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search_tfidf_ngrams.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating the results of the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 21 candidates, totalling 63 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=8, test_size=0.2, train_size=None),\n",
       "             estimator=SVC(random_state=8),\n",
       "             param_grid=[{'C': [0.001, 0.01, 0.1], 'kernel': ['linear'],\n",
       "                          'probability': [True]},\n",
       "                         {'C': [0.001, 0.01, 0.1], 'degree': [3, 4, 5],\n",
       "                          'kernel': ['poly'], 'probability': [True]},\n",
       "                         {'C': [0.001, 0.01, 0.1], 'gamma': [1, 10, 100],\n",
       "                          'kernel': ['rbf'], 'probability': [True]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "C = [.001, .01, .1]\n",
    "degree = [3, 4, 5]\n",
    "gamma = [1, 10, 100]\n",
    "probability = [True]\n",
    "\n",
    "param_grid = [\n",
    "  {'C': C, 'kernel':['linear'], 'probability':probability},\n",
    "  {'C': C, 'kernel':['poly'], 'degree':degree, 'probability':probability},\n",
    "  {'C': C, 'kernel':['rbf'], 'gamma':gamma, 'probability':probability}\n",
    "]\n",
    "\n",
    "# Create a base model\n",
    "svc = svm.SVC(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .2, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=svc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'C': 0.1, 'degree': 4, 'kernel': 'poly', 'probability': True}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.6752136752136751\n"
     ]
    }
   ],
   "source": [
    "#print the best hyperparameters for the model\n",
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "#print the mean accuracy score of the model with the best hyperparameters\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, degree=4, kernel='poly', probability=True, random_state=8)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select and save the best model\n",
    "best_svc = grid_search.best_estimator_\n",
    "#inspect the model\n",
    "best_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model to the training data\n",
    "best_svc.fit(features_train_count, labels_train_count)\n",
    "\n",
    "#get predictions\n",
    "svc_pred = best_svc.predict(features_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - support vector machines - tfidf ngrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        42\n",
      "           1       0.57      1.00      0.73        56\n",
      "\n",
      "    accuracy                           0.57        98\n",
      "   macro avg       0.29      0.50      0.36        98\n",
      "weighted avg       0.33      0.57      0.42        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report - support vector machines - tfidf ngrams\") \n",
    "print(classification_report(labels_test_tfidf_ngrams,svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the new model to the default model for count and tfidf_ngram representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7551020408163265"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = svm.SVC(random_state = 8)\n",
    "base_model.fit(features_train_count, labels_train_count)\n",
    "accuracy_score(labels_test_count, base_model.predict(features_test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc.fit(features_train_count, labels_train_count)\n",
    "accuracy_score(labels_test_count, best_svc.predict(features_test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448979591836735"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = svm.SVC(random_state = 8)\n",
    "base_model.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "accuracy_score(labels_test_tfidf_ngrams, base_model.predict(features_test_tfidf_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "accuracy_score(labels_test_tfidf_ngrams, best_svc.predict(features_test_tfidf_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_classifier_results = base_model.predict(features_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - support vector machines - base model - count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74        42\n",
      "           1       0.83      0.71      0.77        56\n",
      "\n",
      "    accuracy                           0.76        98\n",
      "   macro avg       0.76      0.76      0.75        98\n",
      "weighted avg       0.77      0.76      0.76        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report - support vector machines - base model - count\") \n",
    "print(classification_report(labels_test_count,best_svm_classifier_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SML: K-nearest neighbour\n",
    "   \n",
    "*Important*: The k nearest neighbour classifier performed better on the cleaned text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'algorithm': 'auto',\n",
      " 'leaf_size': 30,\n",
      " 'metric': 'minkowski',\n",
      " 'metric_params': None,\n",
      " 'n_jobs': None,\n",
      " 'n_neighbors': 5,\n",
      " 'p': 2,\n",
      " 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "#defining the algorithm and inspecting its parameters\n",
    "knnc_0 =KNeighborsClassifier()\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(knnc_0.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "n_neighbors = [int(x) for x in np.linspace(start = 1, stop = 300, num = 100)]\n",
    "\n",
    "param_grid = {\"n_neighbors\": n_neighbors}\n",
    "\n",
    "# Create a base model\n",
    "knnc = KNeighborsClassifier()\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .2, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=knnc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring=\"accuracy\",\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search_count = grid_search\n",
    "grid_search_tfidf = grid_search\n",
    "grid_search_tfidf_ngrams = grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search for count vectors are:\n",
      "{'n_neighbors': 19}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.6239316239316239\n"
     ]
    }
   ],
   "source": [
    "grid_search_count.fit(features_train_count, labels_train_count)\n",
    "\n",
    "print(\"The best hyperparameters from Grid Search for count vectors are:\")\n",
    "print(grid_search_tfidf_ngrams.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search_tfidf_ngrams.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search for tfidf vectors are:\n",
      "{'n_neighbors': 124}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.7051282051282052\n"
     ]
    }
   ],
   "source": [
    "grid_search_tfidf.fit(features_train_tfidf, labels_train_tfidf)\n",
    "\n",
    "print(\"The best hyperparameters from Grid Search for tfidf vectors are:\")\n",
    "print(grid_search_tfidf_ngrams.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search_tfidf_ngrams.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search for tfidf vectors with ngrams are:\n",
      "{'n_neighbors': 179}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.7094017094017094\n"
     ]
    }
   ],
   "source": [
    "grid_search_tfidf_ngrams.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "\n",
    "print(\"The best hyperparameters from Grid Search for tfidf vectors with ngrams are:\")\n",
    "print(grid_search_tfidf_ngrams.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search_tfidf_ngrams.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=8, test_size=0.2, train_size=None),\n",
       "             estimator=KNeighborsClassifier(),\n",
       "             param_grid={'n_neighbors': [170, 171, 172, 173, 174, 175, 176, 177,\n",
       "                                         178, 178, 179, 180, 181, 182, 183, 184,\n",
       "                                         185, 186, 187, 188]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors = [170,171,172,173,174,175,176,177,178,178,179,180,181,182,183,184,185,186,187,188]\n",
    "param_grid = {'n_neighbors': n_neighbors}\n",
    "\n",
    "knnc = KNeighborsClassifier()\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .2, random_state = 8)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knnc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=180)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the best model\n",
    "best_knnc = grid_search.best_estimator_\n",
    "#inspect the best model\n",
    "best_knnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=180)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_knnc.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnc_pred = best_knnc.predict(features_test_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - K nearest neighbour - 13 neighbours - count vectors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.71        42\n",
      "           1       0.78      0.77      0.77        56\n",
      "\n",
      "    accuracy                           0.74        98\n",
      "   macro avg       0.74      0.74      0.74        98\n",
      "weighted avg       0.75      0.74      0.75        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report - K nearest neighbour - 13 neighbours - count vectors\") \n",
    "print(classification_report(labels_test_count,best_knnc.predict(features_test_tfidf_ngrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the new model to the default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6530612244897959"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = KNeighborsClassifier()\n",
    "base_model.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "accuracy_score(labels_test_tfidf_ngrams, base_model.predict(features_test_tfidf_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448979591836735"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_knnc.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "accuracy_score(labels_test_tfidf_ngrams, best_knnc.predict(features_test_tfidf_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best classification results for later comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn_classifier_results = best_knnc.predict(features_test_tfidf_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SML: Stochastic Gradient Descent Classifier\n",
    "   \n",
    "*Important:* The SGL classifier performs best with the clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - SGD classifier - count vectors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.52      0.47        42\n",
      "           1       0.57      0.48      0.52        56\n",
      "\n",
      "    accuracy                           0.50        98\n",
      "   macro avg       0.50      0.50      0.50        98\n",
      "weighted avg       0.51      0.50      0.50        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(features_train_count, labels_train_count)\n",
    "count_pred = classifier.predict(features_test_count)\n",
    "print(\"classification report - SGD classifier - count vectors\") \n",
    "print(classification_report(labels_test_count, count_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - SGD classifier - tfidf vectors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.16        42\n",
      "           1       0.58      0.93      0.71        56\n",
      "\n",
      "    accuracy                           0.57        98\n",
      "   macro avg       0.54      0.51      0.44        98\n",
      "weighted avg       0.54      0.57      0.48        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(features_train_tfidf, labels_train_tfidf)\n",
    "tfidf_pred = classifier.predict(features_test_tfidf)\n",
    "print(\"classification report - SGD classifier - tfidf vectors\") \n",
    "print(classification_report(labels_test_tfidf, tfidf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - SGD classifier - tfidf vectors with ngrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.67        42\n",
      "           1       0.74      0.89      0.81        56\n",
      "\n",
      "    accuracy                           0.76        98\n",
      "   macro avg       0.77      0.73      0.74        98\n",
      "weighted avg       0.76      0.76      0.75        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(features_train_tfidf_ngrams, labels_train_tfidf_ngrams)\n",
    "tfidf_ngrams_pred = classifier.predict(features_test_tfidf_ngrams)\n",
    "print(\"classification report - SGD classifier - tfidf vectors with ngrams\") \n",
    "print(classification_report(labels_test_tfidf_ngrams, tfidf_ngrams_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report - naive bayes classifier - MNB - count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64        42\n",
      "           1       0.74      0.57      0.65        56\n",
      "\n",
      "    accuracy                           0.64        98\n",
      "   macro avg       0.65      0.65      0.64        98\n",
      "weighted avg       0.67      0.64      0.64        98\n",
      "\n",
      "\n",
      "classification report - support vector machines - base model - count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74        42\n",
      "           1       0.83      0.71      0.77        56\n",
      "\n",
      "    accuracy                           0.76        98\n",
      "   macro avg       0.76      0.76      0.75        98\n",
      "weighted avg       0.77      0.76      0.76        98\n",
      "\n",
      "\n",
      "classification report - k nearest neighbour - 180 neighbours - tfidf vectors with ngrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.71        42\n",
      "           1       0.78      0.77      0.77        56\n",
      "\n",
      "    accuracy                           0.74        98\n",
      "   macro avg       0.74      0.74      0.74        98\n",
      "weighted avg       0.75      0.74      0.75        98\n",
      "\n",
      "classification report - SGD classifier - tfidf vectors with ngrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.67        42\n",
      "           1       0.74      0.89      0.81        56\n",
      "\n",
      "    accuracy                           0.76        98\n",
      "   macro avg       0.77      0.73      0.74        98\n",
      "weighted avg       0.76      0.76      0.75        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report - naive bayes classifier - MNB - count\") \n",
    "print(classification_report(labels_test_count, best_naive_bayes_classifier_results))\n",
    "print(\"\")\n",
    "print(\"classification report - support vector machines - base model - count\") \n",
    "print(classification_report(labels_test_count,best_svm_classifier_results))\n",
    "print(\"\")\n",
    "print(\"classification report - k nearest neighbour - 180 neighbours - tfidf vectors with ngrams\") \n",
    "print(classification_report(labels_test_count,best_knn_classifier_results))\n",
    "print(\"classification report - SGD classifier - tfidf vectors with ngrams\") \n",
    "print(classification_report(labels_test_tfidf_ngrams, tfidf_ngrams_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
